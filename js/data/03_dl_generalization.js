window.quizData = {
    title: "3-（７）汎化性能向上のためのテクニック",
    
    cheatSheet: `
        <h3>■ 正規化 (Normalization) の使い分け</h3>
        <p>「何を基準に」平均・分散を計算して正規化するかが違います。</p>
        <table>
            <tr><th>手法</th><th>正規化の単位</th><th>主な用途</th></tr>
            <tr><td><strong>Batch Norm</strong></td><td><strong>バッチ方向</strong>（N個のデータ）でまとめる。<br>※学習と推論で挙動が違う。</td><td>CNN (画像)</td></tr>
            <tr><td><strong>Layer Norm</strong></td><td><strong>1つのデータ内</strong>（全チャンネル）でまとめる。<br>バッチサイズに依存しない。</td><td>RNN / Transformer (自然言語)</td></tr>
            <tr><td><strong>Instance Norm</strong></td><td><strong>1つのデータ・1つのチャンネル</strong>でまとめる。<br>コントラスト正規化の効果。</td><td>スタイル変換 (GAN)</td></tr>
            <tr><td><strong>Group Norm</strong></td><td>チャンネルをいくつかの<strong>グループ</strong>に分けてまとめる。<br>Batch Normの「バッチサイズ依存問題」を解決。</td><td>物体検出など</td></tr>
        </table>

        <h3>■ データ拡張 (Data Augmentation)</h3>
        <ul>
            <li><strong>MixUp</strong>: 2枚の画像を重ね合わせ、ラベルも混ぜる（犬0.6、猫0.4など）。</li>
            <li><strong>Random Erasing / Cutout</strong>: 画像の一部をランダムに矩形で塗りつぶす（隠す）。</li>
            <li><strong>EDA (NLP)</strong>: 同義語置換、挿入、削除、入替でテキストデータを増やす。</li>
        </ul>

        <h3>■ アンサンブル & ハイパーパラメータ探索</h3>
        <ul>
            <li><strong>バギング</strong>: 独立したモデルを並列に学習し、多数決/平均をとる（例: Random Forest）。</li>
            <li><strong>ブースティング</strong>: 前のモデルの失敗（誤差）を次のモデルが修正するように直列に学習（例: GBDT）。</li>
            <li><strong>スタッキング</strong>: 複数のモデルの出力を、さらに別のモデル（メタモデル）に入力して最終決定する。</li>
            <li><strong>ベイズ最適化</strong>: 過去の試行結果から「次はどこを探索すべきか」を確率的に推測して探索する。</li>
        </ul>
    `,

    questions: [
        // ---------------------------------------------------------
        // 【基礎編】 Q1 - Q10
        // ---------------------------------------------------------
        {
            category: "Batch Normalization",
            question: "Batch Normalization（バッチ正規化）の主な欠点として、使用が推奨されないケースはどれか。",
            options: ["バッチサイズが極端に小さい場合（例：2〜4）", "画像サイズが大きい場合", "畳み込み層が多い場合", "学習データが多すぎる場合"],
            answer: 0,
            explanation: "バッチ内の平均・分散を使用するため、バッチサイズが小さいと統計量が不安定になり、学習がうまくいきません。"
        },
        {
            category: "Layer Normalization",
            question: "自然言語処理（RNNやTransformer）で、Batch NormよりもLayer Normalizationが好まれる理由は何か。",
            options: ["文の長さが可変であり、バッチごとに統計をとるのが難しいため", "計算コストが高すぎるため", "画像処理専用の技術だから", "過学習しやすいため"],
            answer: 0,
            explanation: "Layer Normは個々のデータ（文）の中で正規化するため、バッチサイズや系列長の影響を受けず、時系列データに適しています。"
        },
        {
            category: "MixUp",
            question: "データ拡張手法「MixUp」において、画像Aと画像Bを 7:3 の比率で混ぜ合わせた時、正解ラベル（One-hotベクトル）はどう処理すべきか。",
            options: ["ラベルも同じ比率（0.7 : 0.3）で混ぜ合わせる（Soft targetにする）", "比率が大きい画像Aのラベル（1.0 : 0）を正解とする", "新しいクラス「AB」を作る", "ラベルは混ぜずにランダムに選ぶ"],
            answer: 0,
            explanation: "「見た目が混ざっているなら、意味（ラベル）も混ざっているはずだ」という考えで、ラベルも線形補間します。これにより過信（確率1.0への張り付き）を防ぎます。"
        },
        {
            category: "バギング",
            question: "アンサンブル学習の「バギング (Bagging)」の説明として正しいものはどれか。",
            options: ["データを復元抽出（ブートストラップ）して複数のモデルを並列に学習させ、最後に多数決や平均をとる", "前のモデルが間違えたデータを重点的に学習させる", "複数のモデルの出力を別のモデルの入力にする", "モデルの層を深くする"],
            answer: 0,
            explanation: "Bootstrap Aggregatingの略。データのばらつきを利用して、モデルの分散（Variance）を減らす効果があります。"
        },
        {
            category: "ブースティング",
            question: "「ブースティング (Boosting)」の特徴として正しいものはどれか。",
            options: ["弱学習器を直列（シーケンシャル）につなぎ、前のモデルの誤りを次のモデルが修正するように学習する", "複数のモデルを独立して学習させ、平均をとる", "データを水増しして学習する", "モデルを圧縮する"],
            answer: 0,
            explanation: "「三人寄れば文殊の知恵」のように、弱いモデルを協力させて強いモデルを作ります。XGBoostやLightGBMが有名です。"
        },
        {
            category: "Random Erasing",
            question: "画像の一部をランダムな矩形で塗りつぶす「Random Erasing」の主な効果は何か。",
            options: ["遮蔽物（オクルージョン）がある場合でも認識できるように、ロバスト性を高める", "画像の色味を変える", "背景を削除する", "計算量を減らす"],
            answer: 0,
            explanation: "「重要なパーツ（猫の顔など）が見えない」状況を人工的に作り出し、それでも認識できるように訓練します。"
        },
        {
            category: "グリッドサーチ",
            question: "ハイパーパラメータ探索手法の一つ「グリッドサーチ」の説明として正しいものはどれか。",
            options: ["あらかじめ決めたパラメータの候補（格子点）を、しらみつぶしに全ての組み合わせで試す", "パラメータをランダムに選んで試す", "確率モデルを使って効率的に探索する", "勾配法を使って探索する"],
            answer: 0,
            explanation: "確実ですが、パラメータ数が増えると組み合わせが爆発的に増えるため、計算コストが非常に高くなります。"
        },
        {
            category: "ベイズ最適化",
            question: "「ベイズ最適化」を用いたハイパーパラメータ探索のメリットは何か。",
            options: ["過去の試行結果から「次によさそうな値」を確率的に予測して探索するため、少ない試行回数で良い値が見つかりやすい", "全ての組み合わせを試すため、必ず最適解が見つかる", "計算が非常に単純で速い", "微分の計算が不要である"],
            answer: 0,
            explanation: "「未知の関数」の形状をガウス過程などで推定しながら、探索（未知の領域）と活用（良さそうな領域）のバランスをとって効率よく探します。"
        },
        {
            category: "Instance Normalization",
            question: "「Instance Normalization」が特によく使われるタスクはどれか。",
            options: ["画像のスタイル変換（画風変換）", "物体検出", "機械翻訳", "音声認識"],
            answer: 0,
            explanation: "画像の「スタイル（コントラストなど）」は画像ごとに異なるため、バッチ全体で正規化せず、画像個別に正規化することでスタイル情報を扱いやすくします。"
        },
        {
            category: "スタッキング",
            question: "アンサンブル手法「スタッキング (Stacking)」の構造はどれか。",
            options: ["1段目の複数のモデルの予測値を特徴量として、2段目のモデル（メタモデル）が最終予測を行う", "複数のモデルのパラメータを平均する", "学習データを分割してそれぞれのモデルで学習する", "モデルの層を積み重ねる"],
            answer: 0,
            explanation: "「モデルの出力」を入力として学習するため、各モデルの得意・不得意まで含めて学習し、精度を底上げします。"
        },

        // ---------------------------------------------------------
        // 【応用編】 Q11 - Q20
        // ---------------------------------------------------------
        {
            category: "正規化の学習と推論(応用)",
            question: "Batch Normalizationにおいて、学習時と推論時で使われる「平均・分散」の扱いの違いとして正しいものはどれか。",
            options: ["学習時は「そのミニバッチの統計量」を使い、推論時は学習中に蓄積した「移動平均」を使う", "学習時も推論時も「その時の入力データの統計量」を使う", "学習時も推論時も「移動平均」を使う", "推論時は正規化を行わない"],
            answer: 0,
            explanation: "推論時にバッチ単位の統計量を使うと、同時に入力されたデータによって結果が変わってしまいます（決定論的でなくなる）。そのため固定された移動平均を使います。"
        },
        {
            category: "Group Normalization(応用)",
            question: "Group Normalizationは、Batch Normのどのような弱点を克服するために考案されたか。",
            options: ["バッチサイズが小さい時に精度が劇的に低下する問題", "計算コストが高い問題", "RNNに適用できない問題", "GPUメモリを使いすぎる問題"],
            answer: 0,
            explanation: "チャンネルをグループに分けてその中で正規化するため、バッチサイズに依存しません。物体検出などバッチサイズを大きくできないタスクで有効です。"
        },
        {
            category: "CutMix(応用)",
            question: "データ拡張「CutMix」は、MixUpとRandom Erasingの利点をどう組み合わせたものか。",
            options: ["画像を重ねる（MixUp）のではなく、画像の一部を切り取って別の画像を貼り付け、ラベルもその面積比で混ぜる", "画像を重ねてから一部を消す", "画像を切り取ってからノイズを加える", "画像を回転させてから混ぜる"],
            answer: 0,
            explanation: "MixUpは画像が重なって不自然（ゴースト）になりますが、CutMixはパッチ貼り付けなので自然な画像構造を保ちつつ、正則化効果を得られます。"
        },
        {
            category: "ランダムサーチの優位性(応用)",
            question: "ハイパーパラメータ探索において、一般に「グリッドサーチ」よりも「ランダムサーチ」の方が効率が良いとされる理由は何か。",
            options: ["重要なパラメータとそうでないパラメータがある場合、ランダムの方が重要なパラメータの探索密度（試せる値の種類）が高くなるから", "運が良いとすぐに当たるから", "計算量が少ないから", "実装が簡単だから"],
            answer: 0,
            explanation: "グリッドサーチは重要でない軸も無駄に細かく探索してしまいますが、ランダムサーチは各軸をバラバラに探索できるため、重要な軸の最適値を見つけやすいです。"
        },
        {
            category: "TTA(応用)",
            question: "コンペなどで使われる「TTA (Test Time Augmentation)」とはどのようなテクニックか。",
            options: ["推論（テスト）時にも画像を反転やズームなどで数パターンに拡張し、それらの予測結果を平均して最終回答とする", "テストデータを学習データに加える", "テスト時間を計測する", "テストデータを圧縮する"],
            answer: 0,
            explanation: "学習時だけでなく、本番でも「ちょっと変えた画像」をいくつか見せて、「平均的に見ればこれだね」と答えることで、精度の安定化を図ります。"
        },
        {
            category: "バギングと分散(応用)",
            question: "バギング（Bagging）が、特に「決定木」などのモデルに対して有効な理由は何か。",
            options: ["決定木は学習データに敏感で分散（バリアンス）が高いモデルだが、平均化することで分散を抑えられるから", "決定木はバイアスが高いモデルだから", "決定木は計算が遅いから", "決定木は過学習しないから"],
            answer: 0,
            explanation: "決定木は少しデータが変わると構造がガラッと変わる不安定なモデルですが、たくさん作って平均することで、非常に強力で安定したモデル（ランダムフォレスト）になります。"
        },
        {
            category: "EDA (NLP)(応用)",
            question: "自然言語処理のデータ拡張手法「EDA (Easy Data Augmentation)」に含まれない操作はどれか。",
            options: ["文法構造の書き換え（構文解析に基づく再構成）", "同義語への置換", "ランダムな単語の挿入", "ランダムな単語の削除"],
            answer: 0,
            explanation: "EDAは深い言語知識を使わず、「表層的な操作（置換・挿入・削除・入替）」だけで手軽にデータを増やす手法です。"
        },
        {
            category: "スタッキングのリーク(応用)",
            question: "スタッキングを行う際、1段目のモデルの予測値を2段目の学習データとして使うが、ここで注意すべき「リーク（Leakage）」を防ぐための手法は何か。",
            options: ["Out-of-Fold (OOF) 予測を使う（学習に使っていないデータに対する予測値だけを集める）", "学習データをそのまま使う", "テストデータを使う", "全データを2回使う"],
            answer: 0,
            explanation: "自分が学習したデータに対する予測値は精度が高すぎて（カンニング状態）、2段目のモデルがそれを過信してしまい、汎化性能が落ちます。必ず「学習に使っていないデータ」での予測値を使います。"
        },
        {
            category: "アンサンブルの多様性(応用)",
            question: "アンサンブル学習の効果を最大化するために、個々のモデルに求められる性質は何か。",
            options: ["それぞれのモデルが、異なる傾向の誤り方をする（多様性がある）こと", "全てのモデルが全く同じ予測をすること", "全てのモデルが完璧な精度を持っていること", "全てのモデルが同じアルゴリズムであること"],
            answer: 0,
            explanation: "全員が同じ間違いをするなら、何人集まっても意味がありません。「違う間違い」をするモデルが集まることで、互いのミスを打ち消し合えます。"
        },
        {
            category: "Whitening(応用)",
            question: "Batch Normalizationの元となったアイデア「白色化 (Whitening)」は、データの相関をなくして分散を1にする強力な前処理だが、なぜCNNの中間層ではあまり使われないのか。",
            options: ["計算コスト（共分散行列の計算と固有値分解）が非常に高く、逆伝播の計算も複雑になるため", "精度が下がるから", "データが増えるから", "実装が不可能だから"],
            answer: 0,
            explanation: "白色化は理想的ですが計算が重すぎます。そこで「相関は無視して、各次元ごとに独立して正規化すればいいや（近似）」としたのがBatch Normのブレイクスルーです。"
        }
    ]
};
