window.quizData = {
    title: "4-（８）（９）様々な学習方法 & 説明性 (XAI)",
    
    cheatSheet: `
        <h3>■ 様々な学習手法</h3>
        <table>
            <tr><th>手法</th><th>概要・特徴</th></tr>
            <tr><td><strong>転移学習</strong><br>(Transfer Learning)</td><td>学習済みモデルの特徴抽出能力を別のタスクに流用する。<br>データが少なくても高性能。<strong>出力層だけ</strong>学習し直すことが多い。</td></tr>
            <tr><td><strong>ファインチューニング</strong><br>(Fine-tuning)</td><td>転移学習の一種だが、<strong>全層</strong>（または一部）の重みを微調整する。<br>よりタスクに特化できるが、データが少ないと過学習しやすい。</td></tr>
            <tr><td><strong>自己教師あり学習</strong><br>(Self-supervised)</td><td>ラベルなしデータから、自分でラベル（正解）を作って学習する。<br>例：<strong>SimCLR</strong>（対照学習）、画像の回転予測。</td></tr>
            <tr><td><strong>距離学習</strong><br>(Metric Learning)</td><td>「似たものは近く、違うものは遠く」なるよう埋め込み空間を学習。<br>例：<strong>Siamese Network</strong>, <strong>Triplet Loss</strong>。</td></tr>
        </table>

        <h3>■ 深層学習の説明性 (XAI)</h3>
        <table>
            <tr><th>手法</th><th>仕組み</th></tr>
            <tr><td><strong>CAM / Grad-CAM</strong></td><td>CNNの最後の畳み込み層の反応を見て、<strong>「画像のどこを見て判断したか」</strong>をヒートマップで可視化する。<br>Grad-CAMは勾配情報を使うため、どんなCNNにも適用可能。</td></tr>
            <tr><td><strong>LIME</strong></td><td>入力画像の周りを少し変化させたデータを生成し、線形モデルで<strong>局所的に近似</strong>して説明する。</td></tr>
            <tr><td><strong>SHAP</strong></td><td><strong>協力ゲーム理論（シャープレイ値）</strong>に基づき、各特徴量が予測にどれだけ貢献したかを算出する。<br>計算コストは高いが、数学的に公平な値が出る。</td></tr>
        </table>
    `,

    questions: [
        // ---------------------------------------------------------
        // 【基礎編】 Q1 - Q10
        // ---------------------------------------------------------
        {
            category: "転移学習",
            question: "大規模な画像データセット（ImageNetなど）で学習済みのモデルを使い、出力層だけを自分のタスク用に付け替えて学習させる手法を一般に何と呼ぶか。",
            options: ["転移学習 (Transfer Learning)", "アンサンブル学習", "強化学習", "メタ学習"],
            answer: 0,
            explanation: "「猫の特徴」を知っているモデルなら、「犬の特徴」も捉えやすいはず、という考え方で、学習済みモデル（特徴抽出器）を再利用します。"
        },
        {
            category: "ファインチューニング",
            question: "ファインチューニングを行う際、学習率（Learning Rate）は通常どのように設定すべきか。",
            options: ["事前学習時よりも小さな値に設定する", "事前学習時よりも大きな値に設定する", "事前学習時と同じ値にする", "ランダムに設定する"],
            answer: 0,
            explanation: "すでに良い特徴を獲得している重みを壊さないように、非常に小さな学習率で慎重に微調整を行います。"
        },
        {
            category: "ドメイン適応",
            question: "「ドメイン適応 (Domain Adaptation)」の目的として正しいものはどれか。",
            options: ["学習データ（ソース領域）とテストデータ（ターゲット領域）の分布が異なる場合に、そのズレ（ドメインシフト）を埋める", "画像の解像度を上げる", "モデルの計算速度を上げる", "教師データを自動生成する"],
            answer: 0,
            explanation: "例：「昼間の画像」で学習したモデルを、「夜間の画像」でも使えるように調整する技術です。"
        },
        {
            category: "距離学習",
            question: "2つの入力（画像など）を同じネットワークに通し、その出力ベクトルの距離が「同じクラスなら近く、違うクラスなら遠く」なるように学習するモデル構造はどれか。",
            options: ["Siamese Network（シャムネットワーク）", "GAN", "Autoencoder", "U-Net"],
            answer: 0,
            explanation: "パラメータを共有する2つのネットワーク（双子）を使うことから、結合双生児（シャム双生児）にちなんで名付けられました。顔認証などで使われます。"
        },
        {
            category: "Triplet Loss",
            question: "Triplet Lossを用いた学習において、入力として必要な3つのデータの組み合わせはどれか。",
            options: ["Anchor（基準）, Positive（同じクラス）, Negative（違うクラス）", "Input, Hidden, Output", "Train, Validation, Test", "Image, Label, Noise"],
            answer: 0,
            explanation: "「基準画像」と「正解画像」の距離を近づけ、「基準画像」と「不正解画像」の距離を遠ざけるように学習します。"
        },
        {
            category: "Grad-CAM",
            question: "CNNの判断根拠を可視化する「Grad-CAM」が、従来のCAM（Class Activation Map）より優れている点は何か。",
            options: ["GAP（Global Average Pooling）層を持たないモデルや、どんな構造のCNNにも適用できる", "可視化の解像度が高い", "計算が非常に高速", "学習が不要"],
            answer: 0,
            explanation: "従来のCAMはGAP層直前の重みを使うためモデル構造を改造する必要がありましたが、Grad-CAMは勾配情報を使うため、既存の学習済みモデルにそのまま使えます。"
        },
        {
            category: "LIME",
            question: "説明可能AIの手法「LIME (Local Interpretable Model-agnostic Explanations)」の基本的な考え方はどれか。",
            options: ["複雑なモデルの判断を、ある入力データの周辺（局所）に限って、単純な線形モデルで近似して説明する", "モデルの全パラメータを解析して説明する", "決定木を使ってモデル全体を近似する", "勾配情報を使って説明する"],
            answer: 0,
            explanation: "全体で見れば複雑な境界線でも、ある1点の周りだけ見れば直線（線形）で近似できる、というアイデアです。"
        },
        {
            category: "SHAP",
            question: "SHAP (SHapley Additive exPlanations) は、ゲーム理論におけるどの概念を応用したものか。",
            options: ["シャープレイ値（Shapley Value）", "ナッシュ均衡", "囚人のジレンマ", "ミニマックス法"],
            answer: 0,
            explanation: "「ある特徴量がある時とない時で、予測結果がどう変わったか」を全組み合わせで計算し、その貢献度（報酬）を公平に分配する値です。"
        },
        {
            category: "自己教師あり学習",
            question: "自己教師あり学習における「対照学習 (Contrastive Learning)」の基本的な学習タスクは何か。",
            options: ["同じ画像から作った2つのデータ（正例ペア）を近づけ、違う画像のデータ（負例）を遠ざける", "画像の欠損部分を埋める", "白黒画像をカラーにする", "次の単語を予測する"],
            answer: 0,
            explanation: "SimCLRなどが有名です。ラベルがなくても、「自分自身の変形画像」は自分と同じクラスであるはずだ、という情報を手がかりに特徴表現を学びます。"
        },
        {
            category: "ドメインシフト",
            question: "機械学習モデルの実運用において問題となる「ドメインシフト」とはどのような現象か。",
            options: ["学習時のデータ分布と、運用時のデータ分布がズレてしまい、精度が低下する現象", "学習データが少なすぎる現象", "モデルのサイズが大きすぎる現象", "ラベル付けが間違っている現象"],
            answer: 0,
            explanation: "例：「綺麗な商品画像」で学習したのに、現場では「スマホで撮ったブレた画像」が入力されて認識できない、など。"
        },

        // ---------------------------------------------------------
        // 【応用編】 Q11 - Q20
        // ---------------------------------------------------------
        {
            category: "転移学習の戦略(応用)",
            question: "転移学習において、新しいタスクのデータ量が「少なく」、かつ元のタスクと「似ている」場合、どの層を学習（更新）させるのが定石か。",
            options: ["出力層（Classifier）のみを学習させ、他の層は凍結（Freeze）する", "全ての層を再学習させる", "入力層に近い層だけを学習させる", "ランダムに層を選んで学習させる"],
            answer: 0,
            explanation: "データが少ないのに全層学習すると過学習します。元のモデルが良い特徴抽出器として機能するため、分類器部分だけすげ替えれば十分です。"
        },
        {
            category: "破滅的忘却(応用)",
            question: "継続学習（Continual Learning）などにおいて、新しいタスクを学習させると、以前学習したタスクの知識が急激に失われる現象を何と呼ぶか。",
            options: ["破滅的忘却 (Catastrophic Forgetting)", "勾配消失", "過学習", "モード崩壊"],
            answer: 0,
            explanation: "ニューラルネットワークの重みが新しいタスク用に上書きされてしまうために起こります。これを防ぐ研究（EWCなど）も進んでいます。"
        },
        {
            category: "Triplet Lossの学習(応用)",
            question: "Triplet Lossの学習において、すでに十分に距離が離れていて損失が0になるような簡単な組み合わせ（Easy Triplets）ばかり学習させるとどうなるか。",
            options: ["勾配が発生せず、学習が進まないため、学習効率が悪い", "過学習してしまう", "非常に高い精度が出る", "計算エラーになる"],
            answer: 0,
            explanation: "学習に寄与するのは、識別が難しい「Hard Triplets」や「Semi-Hard Triplets」です。これらをうまく選ぶ（マイニングする）ことが重要です。"
        },
        {
            category: "Grad-CAMの計算(応用)",
            question: "Grad-CAMにおいて、特徴マップの重み付け係数（どの特徴マップが重要か）を求めるために使われる値は何か。",
            options: ["出力クラススコアに対する、最後の畳み込み層の特徴マップの勾配の「平均値」", "特徴マップの最大値", "全結合層の重みそのもの", "入力画像の画素値"],
            answer: 0,
            explanation: "「この特徴マップの値が上がると、クラススコアがどれくらい上がるか（勾配）」をマップ全体で平均（GAP）したものを、そのマップの重要度とみなします。"
        },
        {
            category: "SHAPの特徴(応用)",
            question: "SHAPの欠点として挙げられるものはどれか。",
            options: ["特徴量の全ての組み合わせ（部分集合）を考慮する必要があるため、厳密な計算コストが指数関数的に増大する", "線形モデルにしか適用できない", "結果が直感的でない", "画像の可視化ができない"],
            answer: 0,
            explanation: "特徴量が多すぎると計算が終わらないため、実際には近似計算（Kernel SHAPなど）が用いられます。"
        },
        {
            category: "SimCLR(応用)",
            question: "自己教師あり学習「SimCLR」において、高い精度を出すために特に重要だと判明した要素は何か。",
            options: ["強力なデータ拡張（特に色変換と切り抜き）と、大きなバッチサイズ", "深いネットワーク構造", "特殊な損失関数", "ラベル付きデータ"],
            answer: 0,
            explanation: "単なる回転などではなく、色味を大きく変えるなどの強い拡張を行っても同一とみなすよう学習させることで、頑健な特徴表現を獲得できます。"
        },
        {
            category: "敵対的ドメイン適応(応用)",
            question: "GANの考え方を応用した「敵対的ドメイン適応」では、ドメイン識別器（Domain Discriminator）に対して特徴抽出器はどのように学習するか。",
            options: ["入力データが「ソース」か「ターゲット」かを見分けられない（不変な）特徴を抽出するように学習する", "ソースデータだけを分類できるように学習する", "ターゲットデータをソースデータに変換するように学習する", "ドメイン識別器の精度を上げるように学習する"],
            answer: 0,
            explanation: "「どのドメインから来た画像か分からない」ような特徴量こそが、ドメイン共通の（本質的な）特徴である、という考え方です。"
        },
        {
            category: "距離学習の損失関数(応用)",
            question: "Siamese Networkで使われる「Contrastive Loss」は、ペアが同じクラスの場合、距離をどうするように働くか。",
            options: ["距離を0に近づける（最小化する）", "距離をあるマージン以上に広げる", "距離を一定値に保つ", "距離に関係なく損失0にする"],
            answer: 0,
            explanation: "同じクラスなら距離0を目指し、違うクラスならある閾値（マージン）以上離れることを目指します。"
        },
        {
            category: "XAIの脆弱性(応用)",
            question: "説明可能AI（Grad-CAMなど）の信頼性に関する問題点として指摘されていることは何か。",
            options: ["Adversarial Attack（敵対的攻撃）のようなノイズを加えると、予測結果は変わらなくても説明（ヒートマップ）が劇的に変わってしまうことがある", "常に人間と同じ場所を見るため、モデルのミスを発見できない", "計算が速すぎて精度が低い", "白黒画像には使えない"],
            answer: 0,
            explanation: "「説明」もまたモデルの出力の一部であり、入力の微小な変化に対して脆弱である可能性が指摘されています。"
        },
        {
            category: "Meta Learning(応用)",
            question: "「メタ学習 (Meta-Learning)」の目的を一言で言うと何か。",
            options: ["「学習の仕方」を学習し、少数のデータで新しいタスクに素早く適応できるモデルを作る（Learn to Learn）", "メタデータを分析する", "非常に深いモデルを学習する", "学習データを生成する"],
            answer: 0,
            explanation: "MAMLなどが有名です。パラメータの初期値を、「どんなタスクが来ても少しの学習で適応できる場所」に持っていく学習などを指します。"
        }
    ]
};
