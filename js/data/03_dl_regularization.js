window.quizData = {
    title: "3-（３）深層モデルのための正則化・汎化性能向上",
    
    cheatSheet: `
        <h3>■ L1正則化 vs L2正則化</h3>
        <p>損失関数 $E(w)$ に「ペナルティ項」を加えて重みを小さくする手法です。</p>
        <table>
            <tr><th>項目</th><th>L1正則化 (Lasso)</th><th>L2正則化 (Ridge)</th></tr>
            <tr><td><strong>ペナルティ項</strong></td><td>$\\lambda \\sum |w|$ (重みの絶対値)</td><td>$\\frac{1}{2} \\lambda \\sum w^2$ (重みの二乗)</td></tr>
            <tr><td><strong>微分係数</strong></td><td>$\\lambda \\cdot \\text{sign}(w)$ (定数)</td><td>$\\lambda w$ (重みに比例)</td></tr>
            <tr><td><strong>特徴</strong></td><td>重みが完全に<strong>0になりやすい</strong>。<br>→ <strong>変数選択</strong>（不要な特徴削除）に有効。</td><td>重みが0にはならず、全体的に小さくなる。<br>→ 過学習抑制の標準手法（Weight Decay）。</td></tr>
        </table>

        <h3>■ その他の重要テクニック</h3>
        <ul>
            <li><strong>ドロップアウト</strong>: 学習時にニューロンをランダムに消す。<br>→ <strong>アンサンブル学習</strong>と同様の効果が得られる。<br>※推論時は全ニューロンを使い、出力を $(1-p)$ 倍する。</li>
            <li><strong>バッチ正規化</strong>: 各層の入力を「平均0、分散1」に整える。<br>→ 学習が<strong>高速化</strong>し、初期値依存性が減る。</li>
            <li><strong>早期終了 (Early Stopping)</strong>: 検証誤差が下がらなくなったら学習を止める。<br>→ 無駄な学習（過学習）を防ぐ。</li>
            <li><strong>データ拡張</strong>: 画像を回転・反転させてデータを擬似的に増やす。<br>→ 汎化性能を直接的に高める最強の手法。</li>
        </ul>
    `,

    questions: [
        // ---------------------------------------------------------
        // 【基礎編】 Q1 - Q10
        // ---------------------------------------------------------
        {
            category: "L1/L2正則化",
            question: "L1正則化（Lasso）の最大の特徴はどれか。",
            options: ["不要な重みが完全に0になりやすく、スパースな解が得られる（変数選択の効果）", "重みが全体的に小さくなるが、0にはなりにくい", "計算時に微分が不可能になるため使われない", "重みの二乗和を損失関数に加える"],
            answer: 0,
            explanation: "L1ノルムの等高線（ひし形）は軸上で接しやすいため、パラメータの一部が完全に0になります。"
        },
        {
            category: "L1/L2正則化",
            question: "L2正則化（Ridge）において、損失関数に加えられるペナルティ項はどれか。",
            options: ["$\\lambda \\sum w^2$", "$\\lambda \\sum |w|$", "$\\lambda \\sum \\log w$", "$\\lambda \\sum \\sqrt{w}$"],
            answer: 0,
            explanation: "重みの二乗和（ユークリッド距離の二乗）を加えます。これをWeight Decay（荷重減衰）とも呼びます。"
        },
        {
            category: "ドロップアウト",
            question: "ドロップアウト（Dropout）の学習時の挙動として正しいものはどれか。",
            options: ["ニューロンをランダムに選んで、その出力を0（無効化）にする", "重みの値をランダムに0にする", "学習データをランダムに捨てる", "勾配の値をランダムに0にする"],
            answer: 0,
            explanation: "中間層のニューロンを確率 $p$ でランダムに「存在しない」ものとして扱い、ネットワークの形を毎回変えながら学習させます。"
        },
        {
            category: "バッチ正規化",
            question: "バッチ正規化（Batch Normalization）の主な効果として**誤っている**ものはどれか。",
            options: ["過学習を促進し、訓練誤差を極限まで小さくする", "学習係数（Learning Rate）を大きく設定でき、学習が高速化する", "重みの初期値への依存性が減る（初期値選びに神経質にならなくて済む）", "正則化の効果（ドロップアウトの代用など）も多少ある"],
            answer: 0,
            explanation: "バッチ正規化は学習を安定させ、過学習を「抑制」する効果があります。「促進」は誤りです。"
        },
        {
            category: "早期終了",
            question: "「早期終了（Early Stopping）」は、どの指標を監視して学習をストップさせるか。",
            options: ["検証データ（Validation）の誤差", "訓練データ（Train）の誤差", "テストデータ（Test）の誤差", "勾配の大きさ"],
            answer: 0,
            explanation: "訓練誤差は下がり続けても、検証誤差が上がり始めたら「過学習」が始まった合図です。そこで止めます。"
        },
        {
            category: "データ拡張",
            question: "画像認識における「データ拡張（Data Augmentation）」の手法として適切でないものはどれか。",
            options: ["画像をランダムな値（ノイズ）のみに置き換える", "画像を左右反転（Flip）させる", "画像を少し回転（Rotation）させる", "画像の一部を切り抜く（Crop）"],
            answer: 0,
            explanation: "画像を完全なノイズにしてしまうと、ラベル（正解）の意味が失われてしまいます。データ拡張は「ラベルの意味を保ったまま」変換する必要があります。"
        },
        {
            category: "Weight Decay",
            question: "最適化手法において「Weight Decay（荷重減衰）」を設定することは、実質的にどの正則化を行うのと同じか。",
            options: ["L2正則化", "L1正則化", "ドロップアウト", "バッチ正規化"],
            answer: 0,
            explanation: "SGDなどの更新式において、重みを毎回少しだけ減衰させる項を加えることは、L2正則化項を微分して引くことと数学的に等価です。"
        },
        {
            category: "正則化の目的",
            question: "正則化（Regularization）を導入する主たる目的は何か。",
            options: ["過学習（Overfitting）を防ぎ、汎化性能を高める", "学習データに対する精度（訓練精度）を最大化する", "計算速度を向上させる", "勾配消失を防ぐ"],
            answer: 0,
            explanation: "モデルが訓練データに適合しすぎるのを防ぐために、あえて制約（ペナルティやノイズ）を加えるのが正則化です。"
        },
        {
            category: "アンサンブル学習",
            question: "ドロップアウトは、概念的にどのような学習手法の近似とみなせるか。",
            options: ["アンサンブル学習（Baggingなど）", "転移学習", "強化学習", "教師なし学習"],
            answer: 0,
            explanation: "毎回異なる形状のネットワークを学習させるため、多数の異なるモデルを平均化する「アンサンブル学習」と同じ効果が得られます。"
        },
        {
            category: "正則化パラメータ",
            question: "正則化項にかかる係数 $\\lambda$ （正則化の強さ）を大きくしすぎると、モデルはどうなるか。",
            options: ["未学習（Underfitting）になる", "過学習（Overfitting）になる", "最適なモデルになる", "計算が終わらなくなる"],
            answer: 0,
            explanation: "ペナルティが強すぎると、重みが限りなく0に近づいてしまい、モデルが単純になりすぎて何も学習できなくなります（高バイアス）。"
        },

        // ---------------------------------------------------------
        // 【応用編】 Q11 - Q20
        // ---------------------------------------------------------
        {
            category: "L1正則化の幾何学(応用)",
            question: "L1正則化によって重みが0になりやすい理由を、幾何学的なイメージ（等高線）で説明したものはどれか。",
            options: ["制約領域が「ひし形（正方形）」であり、その頂点（軸上）で損失関数の等高線と接しやすいため", "制約領域が「円」であり、どこで接しても滑らかだから", "制約領域が非凸集合だから", "L1ノルムは微分不可能だから"],
            answer: 0,
            explanation: " L1の制約領域 $|w_1| + |w_2| \\le C$ は角張ったひし形になるため、確率的に「角（軸上）」で最適解になりやすく、軸上の値は0になります。"
        },
        {
            category: "ドロップアウトの推論時(応用)",
            question: "学習時にドロップアウト率 $p=0.5$ （50%を無効化）で学習したモデルを使って、**推論（予測）**を行う際、各ニューロンの出力に対してどのような操作を行う必要があるか。",
            options: ["出力を $0.5$ 倍する（もしくは学習時に出力を $1/0.5$ 倍しておく）", "出力を $2$ 倍する", "何もしなくてよい", "ランダムに50%のニューロンを消す"],
            answer: 0,
            explanation: "学習時は半分の信号しか通っていないため、推論時に全ニューロンを使うと信号量が倍になってしまいます。バランスを取るために推論時に出力を掛けるか、学習時に割っておく（Inverted Dropout）必要があります。"
        },
        {
            category: "バッチ正規化の副作用(応用)",
            question: "バッチ正規化は「ミニバッチごとの平均・分散」を使って正規化するが、バッチサイズが極端に小さい（例: 2〜4）場合、どのような問題が起こるか。",
            options: ["平均・分散の推定が不安定になり、性能が著しく低下する", "計算時間が長くなる", "過学習しやすくなる", "勾配消失が起きる"],
            answer: 0,
            explanation: "少数のデータで計算した統計量は母集団の分布とかけ離れるため、正規化が正しく機能しません。この場合、Layer Normalizationなどが推奨されます。"
        },
        {
            category: "データ拡張の注意点(応用)",
            question: "数字の「6」や「9」が含まれる手書き文字認識データセット（MNISTなど）において、避けるべきデータ拡張手法はどれか。",
            options: ["180度の回転", "平行移動（Shift）", "拡大縮小（Zoom）", "軽微なノイズ付与"],
            answer: 0,
            explanation: "「6」を180度回転させると「9」になってしまい、ラベル（正解）が変わってしまうため、単純な回転処理は危険です。"
        },
        {
            category: "Batch Normの推論時(応用)",
            question: "バッチ正規化を使用しているモデルで**推論**を行う際、正規化に使う「平均」と「分散」はどこから持ってくるか。",
            options: ["学習中に計算した「移動平均（Moving Average）」を使用する", "推論するデータ（1個または少数）からその場で計算する", "常に0と1を使う", "直前の学習バッチの値を使う"],
            answer: 0,
            explanation: "推論時にバッチ単位の統計量を使うと、同時に入力された他のデータによって結果が変わってしまいます。そのため、学習全体を通じて推定した「移動平均」を使います。"
        },
        {
            category: "ラベルスムージング(応用)",
            question: "過学習抑制テクニックの一つである「ラベルスムージング (Label Smoothing)」とはどのような処理か。",
            options: ["正解ラベル（One-hot）の「1」を少し下げ（例: 0.9）、0の部分を少し上げる処理", "入力画像のノイズを平滑化する処理", "時系列データの変動を滑らかにする処理", "損失関数の凸凹を滑らかにする処理"],
            answer: 0,
            explanation: "正解に対する自信過剰（確率100%を目指す動き）を抑え、汎化性能を高める手法です。[0, 1] ではなく [0.05, 0.95] などを教師データにします。"
        },
        {
            category: "ドロップコネクト(応用)",
            question: "ドロップアウトの派生形である「ドロップコネクト (DropConnect)」は何をランダムに消去するか。",
            options: ["重み（結合）そのもの", "ニューロン（ノード）", "バイアス項", "入力層の値"],
            answer: 0,
            explanation: "ドロップアウトは「点（ノード）」を消しますが、ドロップコネクトは「線（重み）」を消します。より自由度が高い正則化です。"
        },
        {
            category: "L2正則化の微分(応用)",
            question: "L2正則化項 $\\frac{1}{2} \\lambda w^2$ を $w$ で微分した結果は $\\lambda w$ となる。これは重みの更新においてどのような作用をもたらすか。",
            options: ["重みの大きさに比例して、0に近づける力（減衰力）が働く", "重みの符号だけを見て、一定量減らす力が働く", "重みが大きいほど更新量が小さくなる", "重みを振動させる力が働く"],
            answer: 0,
            explanation: "大きな重みほど強くペナルティを受け（強いブレーキがかかり）、小さな重みはあまり影響を受けない、という理にかなった作用をします。"
        },
        {
            category: "白色化(応用)",
            question: "バッチ正規化の元となる考え方である、データの前処理「白色化 (Whitening)」とはどのような処理か。",
            options: ["データを無相関化し、さらに分散を1に揃える処理", "データの平均を0にするだけの処理", "データを0〜1の範囲に収める処理", "画像をモノクロにする処理"],
            answer: 0,
            explanation: "PCAなどで相関をなくした上でスケールを揃える処理です。これをニューラルネットの層ごとに行えるように簡易化したのがBatch Normです。"
        },
        {
            category: "Mixup(応用)",
            question: "データ拡張手法の一つ「Mixup」の説明として正しいものはどれか。",
            options: ["2つの異なる画像を線形補間（重ね合わせ）し、ラベルもその比率で混ぜて新たなデータを作る", "画像の一部をランダムに切り取って別の場所に貼り付ける（CutMix）", "画像の一部を隠す（Random Erasing）", "敵対的サンプル（Adversarial Example）を生成して学習させる"],
            answer: 0,
            explanation: "「犬」と「猫」を50%ずつ混ぜた画像を作り、ラベルも「犬0.5, 猫0.5」とする手法です。決定境界が滑らかになり汎化性能が向上します。"
        }
    ]
};
