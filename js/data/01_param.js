window.quizData = {
    title: "（２）確率モデルにおけるパラメータ推定",
    
    cheatSheet: `
        <h3>主要な推定手法の違い</h3>
        <table>
            <tr><th>手法</th><th>最大化するもの</th><th>特徴</th></tr>
            <tr><td><strong>最尤推定 (MLE)</strong></td><td>尤度 $P(D|\\theta)$</td><td>データに最も適合するパラメータを選ぶ。<br>過学習しやすい。</td></tr>
            <tr><td><strong>MAP推定</strong></td><td>事後確率 $P(D|\\theta)P(\\theta)$</td><td>最尤推定 + <strong>事前確率(正則化)</strong>。<br>過学習を抑える。</td></tr>
            <tr><td><strong>ベイズ推定</strong></td><td>事後分布 $P(\\theta|D)$</td><td>パラメータを「点」ではなく<strong>「分布」</strong>として求める。<br>計算コストが高い。</td></tr>
        </table>
        
        <h3>その他の重要キーワード</h3>
        <ul>
            <li><strong>ベイズの定理</strong>: 結果から原因の確率を逆算する公式。</li>
            <li><strong>ナイーブベイズ</strong>: 特徴量同士が「独立」だと仮定して計算を簡単にする分類器。</li>
            <li><strong>MSE (平均二乗誤差)</strong>: ガウス分布のノイズを仮定した最尤推定と等価。</li>
        </ul>
    `,

    // シラバスカテゴリ: （２）確率・統計 > ⅱ.確率モデルにおけるパラメータ推定
// キーワード: ベイズ則、ナイーブベイズ、平均二乗誤差、対数尤度、ダイバージェンス、最尤推定、MAP推定、ベイズ推定

const questions = [
    // ---------------------------------------------------------
    // 1. ベイズ則・ベイズの定理
    // ---------------------------------------------------------
    {
        category: "ベイズ則",
        question: "ベイズの定理の公式 P(Y|X) = P(X|Y)P(Y) / P(X) において、P(Y) は何と呼ばれるか。",
        options: ["事後確率 (Posterior)", "尤度 (Likelihood)", "事前確率 (Prior)", "周辺尤度 (Marginal Likelihood)"],
        answer: 2,
        explanation: "P(Y)はデータを観測する前の確率なので「事前確率」です。P(Y|X)が事後確率、P(X|Y)が尤度です。"
    },
    // ---------------------------------------------------------
    // 2. 最尤推定 (MLE) & 対数尤度
    // ---------------------------------------------------------
    {
        category: "最尤推定",
        question: "最尤推定(MLE)の説明として、最も適切な数式はどれか。(D:データ, θ:パラメータ)",
        options: ["argmax P(θ|D)", "argmax P(D|θ)", "argmax P(D|θ)P(θ)", "argmax P(D,θ)"],
        answer: 1,
        explanation: "最尤推定は、得られたデータDが生成される確率(尤度) P(D|θ) を最大化するパラメータθを探す手法です。"
    },
    {
        category: "対数尤度",
        question: "最尤推定において、尤度関数の対数をとった「対数尤度」を最大化する主な理由は何か。",
        options: ["計算結果が常に正の値になるようにするため", "確率の「積」を「和」に変換し、アンダーフロー（値が小さくなりすぎて0になること）を防ぐため", "微分不可能にするため", "事前分布の影響を無視するため"],
        answer: 1,
        explanation: "確率は0〜1の値なので、掛け続けると非常に小さくなり計算機で扱えなくなります。対数をとることで掛け算を足し算に変換し、計算を安定させます。"
    },
    // ---------------------------------------------------------
    // 3. 平均二乗誤差 (MSE)
    // ---------------------------------------------------------
    {
        category: "平均二乗誤差",
        question: "回帰問題において、誤差（ノイズ）が「ガウス分布（正規分布）」に従うと仮定して最尤推定を行うことは、数式上何をすることと等価になるか。",
        options: ["平均二乗誤差 (MSE) の最小化", "平均絶対誤差 (MAE) の最小化", "交差エントロピー誤差の最小化", "KLダイバージェンスの最大化"],
        answer: 0,
        explanation: "ガウス分布の対数尤度を計算して整理すると、二乗和の最小化（最小二乗法）の式が導かれます。超頻出ポイントです。"
    },
    {
        category: "平均二乗誤差",
        question: "では、誤差（ノイズ）が「ラプラス分布」に従うと仮定した場合の最尤推定は、何と等価になるか。",
        options: ["平均二乗誤差 (MSE) の最小化", "平均絶対誤差 (MAE) の最小化", "ヒンジ損失の最小化", "交差エントロピー誤差の最小化"],
        answer: 1,
        explanation: "ラプラス分布を仮定すると、絶対値誤差（L1ノルム）の最小化が導かれます。"
    },
    // ---------------------------------------------------------
    // 4. MAP推定 (最大事後確率推定)
    // ---------------------------------------------------------
    {
        category: "MAP推定",
        question: "MAP推定と最尤推定(MLE)の決定的な違いは何か。",
        options: ["MAP推定はパラメータを点ではなく分布として求める", "MAP推定は「尤度」だけでなく「事前確率(事前分布)」も考慮して最大化を行う", "MAP推定は常に最尤推定よりも精度が低い", "MAP推定は計算コストが低い"],
        answer: 1,
        explanation: "MAP推定の式は argmax P(D|θ)P(θ) です。最尤推定に事前確率 P(θ) という「事前の信念・制約」を加えたものです。"
    },
    {
        category: "MAP推定",
        question: "MAP推定において、事前分布に「ガウス分布」を用いることは、機械学習における何と等価か。",
        options: ["L1正則化 (Lasso)", "L2正則化 (Ridge/Weight Decay)", "ドロップアウト", "バッチ正規化"],
        answer: 1,
        explanation: "事前分布としてガウス分布（平均0）を仮定すると、損失関数にL2ノルム（重みの二乗和）が追加される形になり、L2正則化と等価になります。"
    },
    // ---------------------------------------------------------
    // 5. ベイズ推定
    // ---------------------------------------------------------
    {
        category: "ベイズ推定",
        question: "ベイズ推定の特徴として正しい記述はどれか。",
        options: ["パラメータθの値を1つの「点」として特定する", "パラメータθの「事後分布 P(θ|D)」そのものを推定する", "計算量が少ないため、大規模データに最も適している", "事前分布を必要としない"],
        answer: 1,
        explanation: "最尤推定やMAP推定が「点推定（最適化）」であるのに対し、ベイズ推定はパラメータを確率変数として扱い、その分布全体を求めます（積分が必要）。"
    },
    {
        category: "ベイズ推定",
        question: "ベイズ推定において、新しいデータ x_new に対する予測分布 P(x_new | D) を求める式はどれか。",
        options: ["P(x_new | θ_MAP)", "∫ P(x_new | θ) P(θ | D) dθ", "P(x_new | θ_MLE)", "P(x_new | D) P(D)"],
        answer: 1,
        explanation: "得られた事後分布 P(θ|D) で重み付けをして、あり得るすべてのθについて積分（期待値計算）を行います。"
    },
    // ---------------------------------------------------------
    // 6. ナイーブベイズ (Naive Bayes)
    // ---------------------------------------------------------
    {
        category: "ナイーブベイズ",
        question: "ナイーブベイズ分類器が「ナイーブ（単純）」と呼ばれる理由は何か。",
        options: ["計算式が足し算だけだから", "全ての特徴量が互いに「独立」であると仮定しているから", "事前分布を一様分布に固定しているから", "ニューラルネットワークを使わないから"],
        answer: 1,
        explanation: "実際には単語同士に関連（相関）があっても、計算を簡単にするために「独立である（P(x,y)=P(x)P(y))」と大胆に仮定するためです。"
    },
    // ---------------------------------------------------------
    // 7. ダイバージェンス
    // ---------------------------------------------------------
    {
        category: "ダイバージェンス",
        question: "KLダイバージェンス D_KL(P||Q) の性質として正しいものはどれか。",
        options: ["PとQが入れ替わっても値は同じである（対称性）", "PとQが入れ替わると値が変わる（非対称性）", "値は常に負になる", "三角不等式が成り立つ"],
        answer: 1,
        explanation: "KLダイバージェンスは「距離」のような指標ですが、非対称（一方通行）です。対称性を持たせたものはJSダイバージェンスです。"
    },
    {
        category: "ダイバージェンス",
        question: "分類問題で用いられる「交差エントロピー誤差」の最小化は、何をすることと等価か。",
        options: ["正解分布Pと予測分布Qの間の「KLダイバージェンス」の最小化", "予測分布Qのエントロピー最大化", "正解分布Pのエントロピー最小化", "正解分布Pと予測分布Qの間の「ユークリッド距離」の最小化"],
        answer: 0,
        explanation: "交差エントロピー = (真の分布のエントロピー) + (KLダイバージェンス) です。前者は定数なので、交差エントロピー最小化はKLダイバージェンス最小化と同じ意味になります。"
    }
];
};
