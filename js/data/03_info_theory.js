window.quizData = {
    title: "（３）情報理論：エントロピーとダイバージェンス",
    
    cheatSheet: `
        <h3>数式の関係性まとめ</h3>
        <ul>
            <li><strong>自己情報量</strong>: $I(x) = -\\log P(x)$ (確率が低いほど大きい)</li>
            <li><strong>エントロピー (平均情報量)</strong>: $H(X) = E[I(x)] = -\\sum P(x) \\log P(x)$</li>
            <li><strong>条件付きエントロピー</strong>: $H(X|Y)$ ($Y$を知った後の$X$の曖昧さ)</li>
            <li><strong>相互情報量</strong>: $I(X;Y) = H(X) - H(X|Y)$ ($X$と$Y$の相関の強さ)</li>
        </ul>

        <h3>距離尺度 (ダイバージェンス)</h3>
        <table>
            <tr><th>指標</th><th>式・特徴</th><th>用途</th></tr>
            <tr><td><strong>KLダイバージェンス</strong></td><td>$D_{KL}(P||Q) = \\sum P \\log (P/Q)$<br><strong>非対称</strong>、非負。</td><td>最尤推定、VAEの損失関数</td></tr>
            <tr><td><strong>クロスエントロピー</strong></td><td>$H(P,Q) = H(P) + D_{KL}(P||Q)$<br>KLの最小化と等価。</td><td>分類問題の損失関数</td></tr>
            <tr><td><strong>JSダイバージェンス</strong></td><td>KLを平均化して<strong>対称</strong>にしたもの。<br>値が発散しない。</td><td>GANの損失関数</td></tr>
        </table>
    `,

    questions: [
        {
            category: "自己情報量",
            question: "ある事象xが起こる確率をP(x)としたとき、自己情報量I(x)の定義式はどれか。",
            options: ["I(x) = log P(x)", "I(x) = -log P(x)", "I(x) = P(x) log P(x)", "I(x) = 1 / P(x)"],
            answer: 1,
            explanation: "確率は1以下の値なので対数をとると負になります。情報量を正の値にするためにマイナスをつけます。"
        },
        {
            category: "エントロピー",
            question: "エントロピー（平均情報量）H(X) が最大値をとるのは、確率分布P(x)がどのような時か。",
            options: ["特定の値の確率が1で、他が0の時（決定論的）", "全ての事象の確率が等しい時（一様分布）", "正規分布に従う時", "確率が0に近づいた時"],
            answer: 1,
            explanation: "「何が起こるか最も予測がつかない状態」でエントロピーは最大化します。これは全ての事象が均等に起こる（一様分布）場合です。"
        },
        {
            category: "計算問題",
            question: "公正なコイン（表裏の確率が各0.5）を1回投げた時のエントロピーは何ビットか。（対数の底は2とする）",
            options: ["0 bit", "0.5 bit", "1 bit", "2 bit"],
            answer: 2,
            explanation: "H(X) = -(0.5 log2 0.5 + 0.5 log2 0.5) = -(0.5 * -1 + 0.5 * -1) = 1。2択の等確率事象は1ビットの情報を持ちます。"
        },
        {
            category: "KLダイバージェンス",
            question: "KLダイバージェンス(Kullback-Leibler divergence)の性質として、正しいものはどれか。",
            options: ["D(P||Q) = D(Q||P) が成り立つ（対称性）", "常に負の値をとる", "PとQが完全に一致する時、値は0になる", "三角不等式を満たす（距離の公理を満たす）"],
            answer: 2,
            explanation: "KLダイバージェンスは距離のような指標ですが、非対称であり、三角不等式も満たしません。値は常に0以上で、分布が一致する時のみ0になります。"
        },
        {
            category: "クロスエントロピー",
            question: "分類問題の損失関数として「クロスエントロピー誤差」を最小化することは、数学的に何をすることと等価か。",
            options: ["正解分布Pと予測分布Qの間の「KLダイバージェンス」を最小化すること", "予測分布Qのエントロピーを最大化すること", "相互情報量を最大化すること", "正解分布Pのエントロピーを最小化すること"],
            answer: 0,
            explanation: "クロスエントロピー H(P,Q) = H(P) + D_KL(P||Q) です。H(P)は定数なので、これを最小化することはKLダイバージェンスの最小化と同じです。"
        },
        {
            category: "JSダイバージェンス",
            question: "JSダイバージェンス(Jensen-Shannon)がKLダイバージェンスと比較して優れている点（特徴）は何か。",
            options: ["計算が簡単である", "PとQを入れ替えても値が変わらない（対称性を持つ）", "値が負になることがある", "正規分布同士でしか計算できない"],
            answer: 1,
            explanation: "JSダイバージェンスはKLを平均化して対称性を持たせた指標です。また、値が常に有限の範囲（0〜1など）に収まるのも特徴です。"
        },
        {
            category: "相互情報量",
            question: "相互情報量 I(X;Y) の定義式として、エントロピー H(X), H(X|Y) を用いた正しいものはどれか。",
            options: ["I(X;Y) = H(X) + H(X|Y)", "I(X;Y) = H(X) - H(X|Y)", "I(X;Y) = H(X|Y) - H(X)", "I(X;Y) = H(X) * H(X|Y)"],
            answer: 1,
            explanation: "相互情報量は「Yを知ることで、Xの不確実性（エントロピー）がどれだけ減ったか」を表します。つまり元の不確実性 H(X) から、Yを知った後の不確実性 H(X|Y) を引いたものです。"
        },
        {
            category: "相互情報量",
            question: "確率変数XとYが互いに「独立」であるとき、相互情報量 I(X;Y) の値はどうなるか。",
            options: ["0になる", "無限大になる", "1になる", "H(X)と等しくなる"],
            answer: 0,
            explanation: "独立している場合、片方を知ってももう片方の情報は何も得られないため、共有する情報量は0になります。"
        },
        {
            category: "結合エントロピー",
            question: "結合エントロピー H(X,Y) を条件付きエントロピーを使って分解した式（連鎖律）はどれか。",
            options: ["H(X,Y) = H(X) + H(Y)", "H(X,Y) = H(X) + H(Y|X)", "H(X,Y) = H(X|Y) + H(Y|X)", "H(X,Y) = H(X) - H(Y)"],
            answer: 1,
            explanation: "「XとY両方の不確実性」は、「まずXの不確実性」＋「Xがわかった状態でのYの不確実性」の和になります。"
        },
        {
            category: "応用",
            question: "決定木分析において、データを分割する際によく用いられる「情報利得（Information Gain）」は、情報理論のどの指標と等しいか。",
            options: ["自己情報量", "相互情報量", "KLダイバージェンス", "クロスエントロピー"],
            answer: 1,
            explanation: "情報利得は「分割前後でのエントロピーの減少量」であり、これは分割条件と目的変数の間の相互情報量と等価です。"
        },
        {
            category: "用語",
            question: "対数の底が 2 のとき情報量の単位は「ビット(bit)」だが、底がネイピア数 e のときの単位は何か。",
            options: ["バイト (byte)", "ナット (nat)", "バン (ban)", "ハートレー (hartley)"],
            answer: 1,
            explanation: "自然対数を使う場合の単位は nat（ナット）です。機械学習の損失関数計算では通常こちらが使われます。"
        },
        {
            category: "条件付きエントロピー",
            question: "条件付きエントロピー H(X|Y) の意味として適切なものはどれか。",
            options: ["Yを知ったもとでのXの平均情報量（不確実性の残り）", "XとYが同時に起こる確率の平均", "Xを知ったことによるYの情報量の増加分", "Yの情報量からXの情報量を引いたもの"],
            answer: 0,
            explanation: "Yという条件（ヒント）が与えられた状態で、なおXに関して残っている不確実性を表します。"
        }
    ]
};
