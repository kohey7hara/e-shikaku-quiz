window.quizData = {
    title: "１．数学的基礎：確率・統計 & 情報理論",
    
    cheatSheet: `
        <h3>■ 確率・統計：パラメータ推定</h3>
        <p>数式だけでなく「イメージ」で区別することが重要です。</p>
        <table>
            <tr><th>用語</th><th>イメージ・試験のツボ</th></tr>
            <tr><td><strong>ベイズの定理</strong></td><td><strong>「逆算の公式」</strong><br>結果から原因の確率を求める。<br>式：$P(Y|X) \\propto P(X|Y)P(Y)$</td></tr>
            <tr><td><strong>最尤推定 (MLE)</strong></td><td><strong>「データ至上主義」</strong><br>尤度（データへの当てはまり）だけを最大化。<br>欠点：<strong>過学習</strong>しやすい。</td></tr>
            <tr><td><strong>MAP推定</strong></td><td><strong>「データ ＋ 事前の思い込み」</strong><br>尤度 × <strong>事前確率</strong> を最大化。<br>事前確率が<strong>正則化</strong>（ブレーキ）の役割を果たす。</td></tr>
            <tr><td><strong>ベイズ推定</strong></td><td><strong>「一点張りしない」</strong><br>答えを点ではなく<strong>「分布」</strong>で出す。<br>積分計算が必要でコストが高い。</td></tr>
            <tr><td><strong>平均二乗誤差 (MSE)</strong></td><td><strong>「ガウス分布の親戚」</strong><br>誤差が<strong>ガウス分布</strong>に従うと仮定した最尤推定と同じ。</td></tr>
        </table>

        <h3>■ 情報理論：エントロピー・距離</h3>
        <table>
            <tr><th>用語</th><th>イメージ・試験のツボ</th></tr>
            <tr><td><strong>自己情報量</strong></td><td><strong>「驚きの大きさ」</strong><br>確率が低い（レア）ほど値は大きい。常にプラス。</td></tr>
            <tr><td><strong>エントロピー</strong></td><td><strong>「予測のつかなさ（乱雑さ）」</strong><br><strong>一様分布</strong>（どれが出るか不明）で最大。<br>確率100%（バレバレ）で 0。</td></tr>
            <tr><td><strong>KLダイバージェンス</strong></td><td><strong>「分布間の距離（一方通行）」</strong><br><strong>非対称</strong> ($P||Q \\neq Q||P$)。<br>常に 0 以上。</td></tr>
            <tr><td><strong>クロスエントロピー</strong></td><td><strong>「KLの相棒」</strong><br>これを最小化 ＝ KLダイバージェンスの最小化。</td></tr>
            <tr><td><strong>相互情報量</strong></td><td><strong>「ヒントの価値」</strong><br>$Y$を知って$X$の謎がどれだけ解けたか。<br>独立なら <strong>0</strong>。</td></tr>
        </table>
    `,

    questions: [
        // ---------------------------------------------------------
        // 【基礎編】 Q1 - Q10
        // ---------------------------------------------------------
        {
            category: "ベイズ則",
            question: "ある病気の罹患率は1%、検査の感度(病気を発見できる確率)は90%、特異度(健康を正しく判定する確率)は90%である。検査で陽性が出たとき、実際に病気である確率は約何%か。",
            options: ["約 8.3%", "約 50%", "約 90%", "約 99%"],
            answer: 0,
            explanation: "陽性になるのは「病気で陽性(1%×0.9)」＋「健康で偽陽性(99%×0.1)」。<br>計算式: $\\frac{0.009}{0.009 + 0.099} = \\frac{0.009}{0.108} \\approx 0.083$ (8.3%)。"
        },
        {
            category: "パラメータ推定",
            question: "最尤推定(MLE)とMAP推定(最大事後確率推定)の違いに関する記述として正しいものはどれか。",
            options: ["MLEは事前分布を考慮するが、MAP推定は考慮しない", "MAP推定は事前分布を考慮するため、データが少ない時の過学習を抑制しやすい", "MLEはベイズ推定の一種であり、事後分布全体を求める", "データ数が無限大になっても、両者の推定結果は一致しない"],
            answer: 1,
            explanation: "MAP推定は「尤度 × 事前確率」を最大化します。事前確率が正則化項（重みの暴走を防ぐ制約）として働くため、過学習に強くなります。"
        },
        {
            category: "ナイーブベイズ",
            question: "ナイーブベイズ分類器が「ナイーブ（単純）」と呼ばれる所以は何か。",
            options: ["特徴量同士が互いに「独立」であると仮定しているから", "線形モデルしか扱えないから", "事前確率を一様分布に固定しているから", "計算に微分を用いないから"],
            answer: 0,
            explanation: "実際には相関があるデータ（例: 「AI」と「人工知能」という単語）でも、計算を簡単にするために「無関係（独立）」とみなして確率を掛け算します。"
        },
        {
            category: "平均二乗誤差",
            question: "回帰問題で「平均二乗誤差(MSE)」を最小化することは、誤差がどのような確率分布に従うと仮定した最尤推定と等価か。",
            options: ["ラプラス分布", "ガウス分布（正規分布）", "ベルヌーイ分布", "ポアソン分布"],
            answer: 1,
            explanation: "ガウス分布 $N(\\mu, \\sigma^2)$ の対数尤度を計算すると、$-(y - t)^2$ の項が出てきます。これを最大化＝二乗誤差を最小化、となります。"
        },
        {
            category: "自己情報量",
            question: "確率1/2で表が出るコインと、確率1/6で1が出るサイコロがある。「サイコロで1が出た」という情報の自己情報量は、「コインで表が出た」情報の何倍か（底は2とする）。",
            options: ["小さい", "大きい", "同じ", "計算不能"],
            answer: 1,
            explanation: "確率は $1/6 < 1/2$ です。確率は低い（レアな）ほど「驚き（情報量）」は大きくなります。<br>具体的には $-\\log_2(1/6) \\approx 2.58$ bit、$-\\log_2(1/2) = 1$ bit です。"
        },
        {
            category: "エントロピー",
            question: "確率変数 $X$ のエントロピー $H(X)$ が最大値をとるのは、確率分布がどのような状態のときか。",
            options: ["ある1つの事象の確率が100%のとき", "全ての事象の確率が等しい（一様分布）とき", "正規分布に従うとき", "データ数が無限大のとき"],
            answer: 1,
            explanation: "エントロピーは「予測のしにくさ（乱雑さ）」を表します。どれが出るか全くわからない（全て等確率）の時が最も乱雑で、エントロピーは最大になります。"
        },
        {
            category: "クロスエントロピー",
            question: "真の分布 $P$ と予測分布 $Q$ のクロスエントロピー $H(P,Q)$ を最小化することは、数学的に何と等価か。",
            options: ["$P$ のエントロピー $H(P)$ の最小化", "$Q$ のエントロピー $H(Q)$ の最大化", "$P$ と $Q$ の KLダイバージェンス $D_{KL}(P||Q)$ の最小化", "相互情報量 $I(P;Q)$ の最大化"],
            answer: 2,
            explanation: "式: $H(P,Q) = H(P) + D_{KL}(P||Q)$。学習データのエントロピー $H(P)$ は定数なので、これを減らすことはKLダイバージェンス（距離）を減らすことと同じです。"
        },
        {
            category: "KLダイバージェンス",
            question: "KLダイバージェンス $D_{KL}(P||Q)$ の性質として誤っているものはどれか。",
            options: ["常に0以上の値をとる", "分布 $P$ と $Q$ が一致するとき 0 になる", "$P$ と $Q$ を入れ替えても値は同じである（対称性）", "三角不等式を満たさない"],
            answer: 2,
            explanation: "KLダイバージェンスは「非対称」です。$D_{KL}(P||Q) \\neq D_{KL}(Q||P)$ となるため、厳密な意味での「距離」ではありません。"
        },
        {
            category: "相互情報量",
            question: "相互情報量 $I(X;Y)$ の定義式として正しいものはどれか。",
            options: ["$I(X;Y) = H(X) + H(X|Y)$", "$I(X;Y) = H(X) - H(X|Y)$", "$I(X;Y) = H(X|Y) - H(X)$", "$I(X;Y) = H(X) \\times H(X|Y)$"],
            answer: 1,
            explanation: "相互情報量は「$Y$を知ったことで、$X$の不確実性($H(X)$)がどれだけ減ったか($H(X|Y)$を引く)」を表します。"
        },
        {
            category: "JSダイバージェンス",
            question: "JSダイバージェンスがKLダイバージェンスと比較して持つ利点は何か。",
            options: ["計算コストが低い", "対称性を持ち、値が有限（0〜1など）に収まる", "負の値をとることができる", "微分が不可能である"],
            answer: 1,
            explanation: "KLの欠点（非対称、無限大に発散する可能性）を解消するために、平均分布を使って対称化・安定化させた指標です。GANなどで使われます。"
        },

        // ---------------------------------------------------------
        // 【応用編】 Q11 - Q20
        // ---------------------------------------------------------
        {
            category: "最尤推定(応用)",
            question: "最尤推定(MLE)に関する記述として、**最も不適切なもの**はどれか。",
            options: ["データ数 $N$ が少ない場合、過剰適合（オーバーフィッティング）しやすい", "対数尤度を最大化しても、解（$\\theta$）は変わらない", "事前分布を一様分布と仮定したMAP推定と、結果は等価になる", "パラメータ $\\theta$ の事後分布 $P(\\theta|D)$ 全体を求める手法である"],
            answer: 3,
            explanation: "分布全体を求めるのは「ベイズ推定」です。最尤推定は一点（点推定）を求める手法です。"
        },
        {
            category: "対数尤度(応用)",
            question: "最尤推定において、尤度そのものではなく「対数尤度」を最大化する理由として、**適切でないもの**はどれか。",
            options: ["確率の積を和に変換でき、微分が容易になるから", "0に近い確率の積によるアンダーフローを防ぐため", "対数関数は単調増加であり、最大値の場所が変わらないから", "対数をとることで、パラメータ $\\theta$ が負の値をとることを防げるから"],
            answer: 3,
            explanation: "対数をとることと、パラメータが負になるのを防ぐことは無関係です（尤度は確率なので常に正ですが、パラメータ自体は負もありえます）。"
        },
        {
            category: "MAP推定(応用)",
            question: "MAP推定におけるデータ数 $N$ と推定結果の関係について、正しい記述はどれか。",
            options: ["データ数 $N$ が増えるにつれて、事前分布の影響が強くなる", "データ数 $N$ が無限大に近づくと、推定結果は最尤推定(MLE)の結果に近づく", "データ数 $N$ が少ないほど、事前分布の影響は無視できるようになる", "事前分布としてガウス分布を用いた場合、推定値は常に0になる"],
            answer: 1,
            explanation: "データが増えれば増えるほど「事実（尤度）」の力が強くなり、「事前の思い込み（事前分布）」の影響は薄れ、MLEと一致していきます。"
        },
        {
            category: "クロスエントロピー(応用)",
            question: "2値分類タスクで「クロスエントロピー誤差」を最小化することは、出力がどの分布に従うと仮定した最尤推定と等価か。",
            options: ["ガウス分布", "ベルヌーイ分布", "ポアソン分布", "一様分布"],
            answer: 1,
            explanation: "コイン投げと同じ「0か1か」の分布はベルヌーイ分布です。この対数尤度をとるとクロスエントロピーの式になります。"
        },
        {
            category: "KLダイバージェンス(応用)",
            question: "KLダイバージェンスの非対称性（$D_{KL}(P||Q) \\neq D_{KL}(Q||P)$）に関する説明として、正しいイメージはどれか。",
            options: ["山登りの「上り」と「下り」で消費カロリー（距離）が違うようなものである", "$P$と$Q$の平均をとれば、距離は0になる", "$P$から$Q$への距離と、$Q$から$P$への距離は常に等しい", "どのような分布であっても、差は定数となる"],
            answer: 0,
            explanation: "基準となる分布がどちらかによって値が変わります。これが行きと帰りでコストが違うイメージです。"
        },
        {
            category: "相互情報量(応用)",
            question: "確率変数 $X$ と $Y$ が互いに「独立」であるとき、相互情報量 $I(X;Y)$ の値はどうなるか。",
            options: ["無限大になる", "1になる", "0になる", "$H(X)$ と等しくなる"],
            answer: 2,
            explanation: "独立とは「無関係」という意味です。片方を知ってもヒントにならないため、得られる情報量（相互情報量）はゼロです。"
        },
        {
            category: "ベイズ推定(応用)",
            question: "ベイズ推定（パラメータの事後分布を求める手法）のデメリットとして、最も適切なものはどれか。",
            options: ["過学習を起こしやすい", "点推定しかできないため、不確実性がわからない", "積分計算が必要となり、計算コストが非常に高い", "事前分布を設定することができない"],
            answer: 2,
            explanation: "分布の面積を1にするためなどに複雑な積分が必要となり、解析的に解けないことが多いため、MCMCなどの計算手法が必要になります。"
        },
        {
            category: "情報理論の単位(応用)",
            question: "対数の底として「2」を用いた場合の単位は「ビット(bit)」だが、底として「ネイピア数 $e$」を用いた場合の単位は何か。",
            options: ["バイト (byte)", "ナット (nat)", "バン (ban)", "ハートレー (hartley)"],
            answer: 1,
            explanation: "自然対数 $\\ln$ を使う場合は nat（ナット）です。機械学習の理論計算ではこちらが主流です。"
        },
        {
            category: "正則化(応用)",
            question: "MAP推定において、事前分布として「ラプラス分布」を仮定することは、機械学習の損失関数にどの項を加えることと等価か。",
            options: ["L1正則化項 (Lasso)", "L2正則化項 (Ridge)", "エントロピー項", "交差検証項"],
            answer: 0,
            explanation: "ラプラス分布 $\\exp(-|x|)$ は絶対値を含むため、L1正則化（重みのスパース化）に対応します。ガウス分布ならL2です。"
        },
        {
            category: "条件付きエントロピー(応用)",
            question: "一般に、条件付きエントロピー $H(X|Y)$ と元のエントロピー $H(X)$ の大小関係として正しいものはどれか（$Y$を知ることで情報が増えることはないと仮定）。",
            options: ["$H(X|Y) \\ge H(X)$", "$H(X|Y) \\le H(X)$", "$H(X|Y) = 0$", "$H(X|Y) = 1$"],
            answer: 1,
            explanation: "ヒント $Y$ を得て、不確実性（エントロピー）が増えることはありません。必ず「減る」か、無関係なら「変わらない（等しい）」かです。"
        }
    ]
};
