window.quizData = {
    title: "2-（１）機械学習の基礎 Vol.1：パターン認識・課題",
    
    cheatSheet: `
        <h3>■ 距離・類似度：試験で問われる「使い分け」</h3>
        <p>データの性質に合わせて、どの距離を使うべきかが問われます。</p>
        <table>
            <tr><th>名称</th><th>数式・定義</th><th>脳内イメージ・特徴</th></tr>
            <tr>
                <td><strong>ユークリッド距離</strong><br>($L_2$ノルム)</td>
                <td>$\\sqrt{\\sum (x_i - y_i)^2}$</td>
                <td><strong>「定規で測った直線距離」</strong><br>・最も一般的。<br>・最短距離を行くイメージ。</td>
            </tr>
            <tr>
                <td><strong>マンハッタン距離</strong><br>($L_1$ノルム)</td>
                <td>$\\sum |x_i - y_i|$</td>
                <td><strong>「碁盤の目の移動距離」</strong><br>・タクシーがビル街を走る距離。<br>・軸に沿ってカクカク進む。</td>
            </tr>
            <tr>
                <td><strong>コサイン距離</strong><br>(1 - 類似度)</td>
                <td>$1 - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{|\\mathbf{x}| |\\mathbf{y}|}$</td>
                <td><strong>「方向（角度）の違い」</strong><br>・ベクトルの<strong>長さは無視</strong>。<br>・文章の類似度などで活躍。</td>
            </tr>
            <tr>
                <td><strong>マハラノビス距離</strong></td>
                <td>$\\sqrt{(\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu})}$</td>
                <td><strong>「分布の広がりを考慮した距離」</strong><br>・分散で割って正規化するイメージ。<br>・異常検知によく使われる。</td>
            </tr>
        </table>

        <h3>■ 学習の課題：バイアス・バリアンスのトレードオフ</h3>
        <p>モデルの複雑さと誤差の関係を表す最重要概念です。</p>
        <table>
            <tr><th>状態</th><th>バイアス<br><small>(思い込み)</small></th><th>バリアンス<br><small>(変動)</small></th><th>モデルの特徴</th></tr>
            <tr>
                <td><strong>未学習</strong><br>(Underfitting)</td>
                <td style="color:red; font-weight:bold;">高い (High)</td>
                <td style="color:blue;">低い (Low)</td>
                <td><strong>「単純すぎる」</strong><br>データの特徴を捉えられていない。<br>例：直線で近似</td>
            </tr>
            <tr>
                <td><strong>適正学習</strong><br>(Just right)</td>
                <td>低〜中</td>
                <td>低〜中</td>
                <td><strong>「ちょうどいい」</strong><br>汎化性能が高い状態。</td>
            </tr>
            <tr>
                <td><strong>過学習</strong><br>(Overfitting)</td>
                <td style="color:blue;">低い (Low)</td>
                <td style="color:red; font-weight:bold;">高い (High)</td>
                <td><strong>「複雑すぎる」</strong><br>ノイズまで学習してしまう。<br>未知のデータに弱い。</td>
            </tr>
        </table>

        <h3>■ その他重要ワード</h3>
        <ul>
            <li><strong>k近傍法 (k-NN)</strong>: 「近くの $k$ 人の多数決」で決める。<br>
                <ul>
                    <li>$k$ が小さい ($k=1$) → 境界が複雑（ギザギザ） → <strong>過学習</strong>しやすい</li>
                    <li>$k$ が大きい ($k=N$) → 境界が単純（平坦） → <strong>未学習</strong>しやすい</li>
                </ul>
            </li>
            <li><strong>次元の呪い</strong>: 次元が増えると空間の体積が爆発的に増え、データがスカスカになる現象。<br>→ 近傍法などが機能しなくなるため、次元削減が必要。</li>
        </ul>
    `,

    questions: [
        // ---------------------------------------------------------
        // 【基礎編】 Q1 - Q10
        // ---------------------------------------------------------
        {
            category: "距離計算",
            question: "2点間の距離を計算する際、各座標の差の絶対値の総和をとるものはどれか。",
            options: ["ユークリッド距離 ($L_2$ノルム)", "マンハッタン距離 ($L_1$ノルム)", "チェビシェフ距離 ($L_\\infty$ノルム)", "マハラノビス距離"],
            answer: 1,
            explanation: "グリッド状の道路を移動する距離に例えられます。式は $\\sum |x_i - y_i|$ です。"
        },
        {
            category: "k近傍法",
            question: "k近傍法(k-NN)において、近傍の数 $k$ を「1」に設定した場合の傾向として正しいものはどれか。",
            options: ["決定境界が滑らかになり、未学習になりやすい", "決定境界が複雑になり、過学習しやすくなる", "すべてのデータを同じクラスに分類する", "計算量が最小になる"],
            answer: 1,
            explanation: "$k=1$ は「最も近い1つのデータ」だけで判断するため、ノイズまで忠実に拾ってしまい、境界が複雑化して過学習のリスクが高まります。"
        },
        {
            category: "機械学習の分類",
            question: "「教師あり学習」に該当しないタスクはどれか。",
            options: ["スパムメールの分類", "住宅価格の予測（回帰）", "顧客データのクラスタリング（グループ化）", "手書き数字認識"],
            answer: 2,
            explanation: "クラスタリングは正解ラベル（教師データ）を与えず、データの構造自体を見つけ出す「教師なし学習」です。"
        },
        {
            category: "過学習",
            question: "モデルが「過学習（Overfitting）」を起こしている時の典型的な状態はどれか。",
            options: ["訓練誤差が大きく、汎化誤差も大きい", "訓練誤差は小さいが、汎化誤差が大きい", "訓練誤差も汎化誤差も小さい", "訓練誤差が大きく、汎化誤差は小さい"],
            answer: 1,
            explanation: "手元のデータ（訓練データ）には完璧に正解するが、未知のデータ（テストデータ）には通用しない状態です。"
        },
        {
            category: "マハラノビス距離",
            question: "マハラノビス距離の特徴として正しいものはどれか。",
            options: ["データの分散や相関（共分散）を考慮して距離を測る", "常にユークリッド距離よりも値が小さくなる", "ベクトルの方向のみを考慮し、大きさは無視する", "計算にデータの平均値を使用しない"],
            answer: 0,
            explanation: "分布の広がりを考慮するため、外れ値検知などによく使われます。共分散行列の逆行列を用いて計算します。"
        },
        {
            category: "次元の呪い",
            question: "「次元の呪い」と呼ばれる現象の説明として適切なものはどれか。",
            options: ["次元が増えると計算時間が指数関数的に減る", "高次元空間ではデータが疎（スカスカ）になり、距離の差がつきにくくなる", "次元削減を行うと必ず情報量が失われる", "高次元データは可視化できないため理解できない"],
            answer: 1,
            explanation: "次元が増えると空間の体積が爆発的に増え、データ間の距離が均一化してしまい、近傍法などが機能しにくくなります。"
        },
        {
            category: "コサイン距離",
            question: "コサイン類似度が「1」になるときの2つのベクトル $\\mathbf{a}, \\mathbf{b}$ の関係はどれか。",
            options: ["向きが完全に同じ（0度）", "直交している（90度）", "向きが正反対（180度）", "長さが等しい"],
            answer: 0,
            explanation: "$\\cos 0^\\circ = 1$ です。コサイン類似度は方向の一致度を表します（コサイン距離は $1 - \\text{類似度}$ なので 0 になります）。"
        },
        {
            category: "半教師あり学習",
            question: "半教師あり学習（Semi-supervised Learning）のデータセット構成として正しいものはどれか。",
            options: ["全てのデータに正解ラベルがついている", "全てのデータに正解ラベルがついていない", "一部のデータにラベルがあり、残りの大量のデータにはラベルがない", "ラベルの代わりに報酬（Reward）が与えられる"],
            answer: 2,
            explanation: "ラベル付けコストが高い場合に、少量のラベル付きデータと大量のラベルなしデータを組み合わせて学習する手法です。"
        },
        {
            category: "バイアス・バリアンス",
            question: "モデルが単純すぎてデータの傾向を捉えられていない（未学習）状態は、バイアスとバリアンスの観点ではどう表現されるか。",
            options: ["高バイアス・低バリアンス", "低バイアス・高バリアンス", "高バイアス・高バリアンス", "低バイアス・低バリアンス"],
            answer: 0,
            explanation: "モデルの思い込みが激しい（単純すぎる）状態は「高バイアス」です。逆にデータに振り回される（複雑すぎる）のが「高バリアンス」です。"
        },
        {
            category: "kd-tree",
            question: "k近傍法の探索を高速化するために用いられる、空間を分割する木構造のアルゴリズムはどれか。",
            options: ["kd-tree", "決定木 (Decision Tree)", "ランダムフォレスト", "B-tree"],
            answer: 0,
            explanation: "k-dimensional treeの略で、空間を軸ごとに分割して近傍探索を効率化します。"
        },

        // ---------------------------------------------------------
        // 【応用編】 Q11 - Q20
        // ---------------------------------------------------------
        {
            category: "k近傍法(応用)",
            question: "k近傍法において、パラメータ $k$ をデータ総数 $N$ と同じ値に設定した場合（$k=N$）、どのような予測結果になるか。",
            options: ["入力データに最も近い1点のクラスが出力される", "常に学習データの中で最も多いクラス（多数派）が出力される", "ランダムなクラスが出力される", "決定境界が非常に複雑になる"],
            answer: 1,
            explanation: "全データとの多数決になるため、入力データの特徴に関わらず、常に全体で最も多いクラスが答えになります（超・高バイアス）。"
        },
        {
            category: "マハラノビス距離(応用)",
            question: "マハラノビス距離において、データの各変数が互いに無相関で、かつ分散が全て「1」の場合、この距離は何と一致するか。",
            options: ["ユークリッド距離", "マンハッタン距離", "コサイン距離", "チェビシェフ距離"],
            answer: 0,
            explanation: "共分散行列が単位行列になるため、式がユークリッド距離と同じ形になります。"
        },
        {
            category: "学習曲線(応用)",
            question: "学習曲線（Learning Curve）において、訓練データのサイズを増やしていった時、訓練誤差と検証誤差が共に高い値で収束し、差が縮まらない場合、何が疑われるか。",
            options: ["過学習 (High Variance)", "未学習 (High Bias)", "適切な学習状態", "データの品質不良"],
            answer: 1,
            explanation: "データが増えても精度が上がらない（誤差が高いまま）なのは、モデルの表現力が不足している「未学習（高バイアス）」の特徴です。"
        },
        {
            category: "次元の呪い(応用)",
            question: "「次元の呪い」への対策として、一般的に行われる前処理はどれか。",
            options: ["データを複製して増やす（オーバーサンプリング）", "次元削減（PCAなど）や特徴量選択を行う", "より高次元の空間へ写像する（カーネル法）", "距離尺度としてユークリッド距離を使う"],
            answer: 1,
            explanation: "不要な次元を削除したり、主成分分析で次元を圧縮することで、密度を確保し学習を効率化します。"
        },
        {
            category: "距離尺度(応用)",
            question: "Lp距離（ミンコフスキー距離）において、$p$ を無限大 ($p \\to \\infty$) に近づけたときの距離は「チェビシェフ距離」と呼ばれるが、これはどのような値になるか。",
            options: ["各座標の差の絶対値の合計", "各座標の差の絶対値のうちの最大値", "各座標の差の絶対値の最小値", "0になる"],
            answer: 1,
            explanation: "$L_\\infty$ノルムは、成分ごとの差の中で「最大の差」だけが支配的になる距離です。"
        },
        {
            category: "k近傍法(応用)",
            question: "k近傍法の特徴として「怠惰学習 (Lazy Learning)」と呼ばれる理由は何か。",
            options: ["計算精度が低いため", "事前にモデルのパラメータ学習を行わず、予測時に初めて全データとの距離計算を行うため", "実装が簡単で手抜きできるため", "学習率の設定が必要ないため"],
            answer: 1,
            explanation: "訓練データを記憶するだけで、モデル（数式）の構築を行いません。そのため学習時間はゼロですが、予測に時間がかかります。"
        },
        {
            category: "バイアス・バリアンス(応用)",
            question: "一般に、モデルの複雑さ（自由度）を上げていくと、バイアスとバリアンスはどのように変化するか。",
            options: ["バイアスは下がり、バリアンスは上がる", "バイアスは上がり、バリアンスは下がる", "両方とも下がる", "両方とも上がる"],
            answer: 0,
            explanation: "モデルが複雑になれば表現力が上がり（低バイアス）、データごとの変動（ノイズ）を拾いやすくなります（高バリアンス）。これはトレードオフの関係です。"
        },
        {
            category: "コサイン距離(応用)",
            question: "テキストマイニングなどで、文書間の類似度を測る際に「ユークリッド距離」よりも「コサイン類似度」が好まれる理由は何か。",
            options: ["計算速度が速いから", "文書の長さ（単語数）の影響を受けずに、単語の出現傾向（ベクトル方向）の一致度を見たいから", "負の値をとらないから", "疎行列に対応していないから"],
            answer: 1,
            explanation: "長い文章と短い文章でも、使われている単語の比率が似ていれば「似ている」と判定したい場合に、大きさ（長さ）を無視できるコサイン類似度が有効です。"
        },
        {
            category: "kd-tree(応用)",
            question: "近傍探索において、kd-treeの検索効率が著しく低下し、全探索と変わらなくなってしまうのはどのような場合か。",
            options: ["データ数が非常に少ない場合", "データの次元数が非常に高い場合", "データが正規分布に従う場合", "k=1の場合"],
            answer: 1,
            explanation: "高次元になると「次元の呪い」により、空間分割による枝刈りの効果が薄れ、ほぼ全てのノードを探索することになってしまいます。"
        },
        {
            category: "正則化(応用)",
            question: "過学習を抑制するための「正則化」は、バイアス・バリアンスの観点ではどのような操作に当たるか。",
            options: ["バリアンスを下げて、バイアスを少し上げる", "バリアンスを上げて、バイアスを下げる", "バリアンスもバイアスも下げる", "バリアンスもバイアスも上げる"],
            answer: 0,
            explanation: "モデルを単純化する（制約をかける）ことで、変動（バリアンス）を抑えますが、その分表現力が落ちるためバイアスはわずかに上昇します。"
        }
    ]
};
